{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Confident AI login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üéâü•≥ Congratulations! You've successfully logged in! üôå \n",
       "</pre>\n"
      ],
      "text/plain": [
       "üéâü•≥ Congratulations! You've successfully logged in! üôå \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "\n",
    "deepeval.login(\"confident_us_mi2F9Prqn81UJrqywO7sqM+pR7W7LS5MepLi4LBtbeE=\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load = load_dotenv('./../.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing simple DeepEval Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy Metrics - Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()\n",
    "test_case = LLMTestCase(\n",
    "  input=\"Who is the current president of the United States of America?\",\n",
    "  actual_output=\"Joe Biden\",\n",
    "  retrieval_context=[\"Joe Biden serves as the current president of America.\"]\n",
    ")\n",
    "\n",
    "answer_relevancy_metric.measure(test_case)\n",
    "print(answer_relevancy_metric.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test using Contextual Precision Metrics - Standalone"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "\n",
    "contextual_precision_metrics = ContextualPrecisionMetric()\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"Who is the current president of USA in 2024\",\n",
    "    # Should come from an LLM or from an Agent or RAG\n",
    "    actual_output=\"Donald Trump\",\n",
    "    # RAG - Vector DB, AI Agent - Agent Tools, LLM - LLM invoke response\n",
    "    retrieval_context=[\"Donald Trump serves as the current president of America.\"],\n",
    "    expected_output=\"Donald Trump is the current president of America.\"\n",
    ")\n",
    "\n",
    "contextual_precision_metrics.measure(test_case=test_case)\n",
    "print(contextual_precision_metrics.score)\n",
    "print(contextual_precision_metrics.success)\n",
    "print(contextual_precision_metrics.score_breakdown)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our Tests without Standalone using - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-\u001B[0m\u001B[1;38;2;55;65;81m4.1\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the question with no irrelevant information. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who is the current president of the United States of America?\n",
      "  - actual output: Joe Biden\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Joe Biden serves as the current president of America.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekks4zj048f2jlogm82esc1/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekks4zj048f2jlogm82esc1/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekks4zj048f2jlogm82esc1/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[38;2;5;245;141m‚úì\u001B[0m Done üéâ! View results on \n",
       "\u001B]8;id=611506;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekks4zj048f2jlogm82esc1/test-cases\u001B\\\u001B[4;94mhttps://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekks4zj048f2jlogm82esc1/test-\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B]8;id=611506;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekks4zj048f2jlogm82esc1/test-cases\u001B\\\u001B[4;94mcases\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and directly addressed the question with no irrelevant information. Great job!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0030800000000000003, verbose_logs='Statements:\\n[\\n    \"Joe Biden\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Who is the current president of the United States of America?', actual_output='Joe Biden', expected_output=None, context=None, retrieval_context=['Joe Biden serves as the current president of America.'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekks4zj048f2jlogm82esc1/test-cases')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()\n",
    "test_case = LLMTestCase(\n",
    "  input=\"Who is the current president of the United States of America?\",\n",
    "  actual_output=\"Joe Biden\",\n",
    "  retrieval_context=[\"Joe Biden serves as the current president of America.\"]\n",
    ")\n",
    "\n",
    "evaluate(test_cases=[test_case], metrics=[answer_relevancy_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-\u001B[0m\u001B[1;38;2;55;65;81m4.1\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚ùå Answer Relevancy (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.00 because the actual output did not answer the question and instead provided irrelevant information about OpenAI, which is not related to who built the Claude Models., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who built the Claude Models?\n",
      "  - actual output: OpenAI\n",
      "  - expected output: Claude Anthrophic\n",
      "  - context: None\n",
      "  - retrieval context: ['Claude Anthrophic built the GPT models.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 50.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the question with no irrelevant information. Great job staying focused and concise!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who is the current president of the United States of America?\n",
      "  - actual output: Joe Biden\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Joe Biden serves as the current president of America.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 50.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmeklhlzn0065vpvcixgl8f8z/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmeklhlzn0065vpvcixgl8f8z/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmeklhlzn0065vpvcixgl8f8z/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[38;2;5;245;141m‚úì\u001B[0m Done üéâ! View results on \n",
       "\u001B]8;id=342749;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmeklhlzn0065vpvcixgl8f8z/test-cases\u001B\\\u001B[4;94mhttps://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmeklhlzn0065vpvcixgl8f8z/test-\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B]8;id=342749;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmeklhlzn0065vpvcixgl8f8z/test-cases\u001B\\\u001B[4;94mcases\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_1', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=False, score=0.0, reason='The score is 0.00 because the actual output did not answer the question and instead provided irrelevant information about OpenAI, which is not related to who built the Claude Models.', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0033099999999999996, verbose_logs='Statements:\\n[\\n    \"OpenAI\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"OpenAI did not build the Claude Models; they were built by Anthropic. The statement is not relevant to answering the question.\"\\n    }\\n]')], conversational=False, multimodal=False, input='Who built the Claude Models?', actual_output='OpenAI', expected_output='Claude Anthrophic', context=None, retrieval_context=['Claude Anthrophic built the GPT models.'], additional_metadata=None), TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and directly addressed the question with no irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.0031219999999999998, verbose_logs='Statements:\\n[\\n    \"Joe Biden.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Who is the current president of the United States of America?', actual_output='Joe Biden', expected_output=None, context=None, retrieval_context=['Joe Biden serves as the current president of America.'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmeklhlzn0065vpvcixgl8f8z/test-cases')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()\n",
    "\n",
    "test_case1 = LLMTestCase(\n",
    "  input=\"Who is the current president of the United States of America?\",\n",
    "  actual_output=\"Joe Biden\",\n",
    "  retrieval_context=[\"Joe Biden serves as the current president of America.\"]\n",
    ")\n",
    "\n",
    "test_case2 = LLMTestCase(\n",
    "  input=\"Who built the Claude Models?\",\n",
    "  actual_output=\"OpenAI\",\n",
    "  expected_output= \"Claude Anthrophic\",\n",
    "  retrieval_context=[\"Claude Anthrophic built the GPT models.\"]\n",
    ")\n",
    "\n",
    "evaluate(test_cases=[test_case1, test_case2], metrics=[answer_relevancy_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate With Golden DataSet and EvaluationDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate\n",
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "# Create Golden instead of Test cases\n",
    "golden = Golden(\n",
    "    input=\"Who is the current president of the United States of America?\",\n",
    "    expected_output=\"Joe Biden\",\n",
    "    context=[\"Joe Biden serves as the current president of America.\"]\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_golden(golden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='Who is the current president of the United States of America?', actual_output=None, expected_output='Joe Biden', context=['Joe Biden serves as the current president of America.'], retrieval_context=None, turns=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None)], _alias=None, _id=None, _multi_turn=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Test Case from Golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-\u001B[0m\u001B[1;38;2;55;65;81m4.1\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the question without any irrelevant information. Great job staying focused and concise!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who is the current president of the United States of America?\n",
      "  - actual output: Joe Biden\n",
      "  - expected output: Joe Biden\n",
      "  - context: None\n",
      "  - retrieval context: ['Joe Biden serves as the current president of America.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekpsq0i026cvpvcb8d5fa2l/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekpsq0i026cvpvcb8d5fa2l/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekpsq0i026cvpvcb8d5fa2l/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[38;2;5;245;141m‚úì\u001B[0m Done üéâ! View results on \n",
       "\u001B]8;id=102220;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekpsq0i026cvpvcb8d5fa2l/test-cases\u001B\\\u001B[4;94mhttps://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekpsq0i026cvpvcb8d5fa2l/test-\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B]8;id=102220;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekpsq0i026cvpvcb8d5fa2l/test-cases\u001B\\\u001B[4;94mcases\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and directly addressed the question without any irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.006103999999999999, verbose_logs='Statements:\\n[\\n    \"Joe Biden\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Who is the current president of the United States of America?', actual_output='Joe Biden', expected_output='Joe Biden', context=None, retrieval_context=['Joe Biden serves as the current president of America.'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmekpsq0i026cvpvcb8d5fa2l/test-cases')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for golden in dataset.goldens:\n",
    "    test_case = LLMTestCase(\n",
    "        input=golden.input,\n",
    "        expected_output=golden.expected_output,\n",
    "        actual_output=\"Joe Biden\",\n",
    "        retrieval_context=golden.context\n",
    "    )\n",
    "    \n",
    "    dataset.add_test_case(test_case)\n",
    "    \n",
    "evaluate(test_cases=dataset.test_cases, metrics=[AnswerRelevancyMetric()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Evaluation Dataset as Goldens in Confident AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"Who is the current president of the United States of America?\",\n",
    "        \"expected_output\": \"Joe Biden\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Who introducted the GPT Model?\",\n",
    "        \"expected_output\": \"Open AI\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='Who is the current president of the United States of America?', actual_output=None, expected_output='Joe Biden', context=None, retrieval_context=None, turns=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None), Golden(input='Who introducted the GPT Model?', actual_output=None, expected_output='Open AI', context=None, retrieval_context=None, turns=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None)], _alias=None, _id=None, _multi_turn=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate\n",
    "from deepeval.dataset import EvaluationDataset, Golden\n",
    "\n",
    "goldens = []\n",
    "\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input= data['input'],\n",
    "        expected_output=data['expected_output'],\n",
    "    )\n",
    "    goldens.append(golden)\n",
    "    \n",
    "new_dataset = EvaluationDataset(goldens=goldens)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push the Dataset to Confident AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/datasets/cmekqc1lx026xvpvc36zgupts\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/datasets/cmekqc1lx026xvpvc36zgupts</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚úÖ Dataset successfully pushed to Confident AI! View at \n",
       "\u001B]8;id=714556;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/datasets/cmekqc1lx026xvpvc36zgupts\u001B\\\u001B[4;94mhttps://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/datasets/cmekqc1lx026xvpvc36zgupts\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_dataset.push(alias=\"TestGoldenDataSet\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='Who is the current president of the United States of America?', actual_output=None, expected_output='Joe Biden', context=None, retrieval_context=None, turns=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None), Golden(input='Who introducted the GPT Model?', actual_output=None, expected_output='Open AI', context=None, retrieval_context=None, turns=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None)], _alias=None, _id=None, _multi_turn=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull the Dataset from Confident AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='Who is the current president of the United States of America?', actual_output=None, expected_output='Donald Trump', context=None, retrieval_context=None, turns=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None), Golden(input='Who introducted the GPT Model?', actual_output=None, expected_output='Open AI', context=None, retrieval_context=None, turns=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None)], _alias=TestGoldenDataSet, _id=cmekqc1lx026xvpvc36zgupts, _multi_turn=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloudDataSet = EvaluationDataset()\n",
    "cloudDataSet.pull(alias=\"TestGoldenDataSet\")\n",
    "cloudDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare our Testcase to evaluate our records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_llms_app(input):\n",
    "    if input == 1:\n",
    "        return \"Joe Biden\"\n",
    "    elif input == 2:\n",
    "        return \"Open AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "counter = 1\n",
    "for golden in cloudDataSet.goldens:\n",
    "    test_case = LLMTestCase(\n",
    "        input= golden.input,\n",
    "        expected_output=golden.expected_output,\n",
    "        actual_output=mock_llms_app(counter),\n",
    "    )\n",
    "    counter += 1\n",
    "    cloudDataSet.add_test_case(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMTestCase(input='Who is the current president of the United States of America?', actual_output='Joe Biden', expected_output='Joe Biden', context=None, retrieval_context=None, additional_metadata=None, tools_called=None, comments=None, expected_tools=None, token_cost=None, completion_time=None, name=None, tags=None, mcp_servers=None, mcp_tools_called=None, mcp_resources_called=None, mcp_prompts_called=None), LLMTestCase(input='Who introducted the GPT Model?', actual_output='Open AI', expected_output='Open AI', context=None, retrieval_context=None, additional_metadata=None, tools_called=None, comments=None, expected_tools=None, token_cost=None, completion_time=None, name=None, tags=None, mcp_servers=None, mcp_tools_called=None, mcp_resources_called=None, mcp_prompts_called=None)]\n"
     ]
    }
   ],
   "source": [
    "print(cloudDataSet.test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing gpt-\u001B[0m\u001B[1;38;2;55;65;81m4.1\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/karthik/tryout /aiqaDemo/myenv312/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the question without any irrelevant information. Great job staying focused and concise!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who introducted the GPT Model?\n",
      "  - actual output: Open AI\n",
      "  - expected output: Open AI\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the question with no irrelevant information. Great job staying focused and concise!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who is the current president of the United States of America?\n",
      "  - actual output: Joe Biden\n",
      "  - expected output: Donald Trump\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel02m2z09312jlom66qhkfo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel02m2z09312jlom66qhkfo/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel02m2z09312jlom66qhkfo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[38;2;5;245;141m‚úì\u001B[0m Done üéâ! View results on \n",
       "\u001B]8;id=616622;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel02m2z09312jlom66qhkfo/test-cases\u001B\\\u001B[4;94mhttps://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel02m2z09312jlom66qhkfo/test-\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B]8;id=616622;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel02m2z09312jlom66qhkfo/test-cases\u001B\\\u001B[4;94mcases\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and directly addressed the question without any irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.003092, verbose_logs='Statements:\\n[\\n    \"Open AI\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Who introducted the GPT Model?', actual_output='Open AI', expected_output='Open AI', context=None, retrieval_context=None, additional_metadata=None), TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer was fully relevant and directly addressed the question with no irrelevant information. Great job staying focused and concise!', strict_mode=False, evaluation_model='gpt-4.1', error=None, evaluation_cost=0.003984, verbose_logs='Statements:\\n[\\n    \"Joe Biden\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Who is the current president of the United States of America?', actual_output='Joe Biden', expected_output='Donald Trump', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel02m2z09312jlom66qhkfo/test-cases')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_cases=cloudDataSet.test_cases, metrics=[AnswerRelevancyMetric()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Local LLM for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üôå Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001B[38;2;106;0;255mAnswer Relevancy Metric\u001B[0m! \u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81musing deepseek-r\u001B[0m\u001B[1;38;2;55;65;81m1:8b\u001B[0m\u001B[38;2;55;65;81m \u001B[0m\u001B[1;38;2;55;65;81m(\u001B[0m\u001B[38;2;55;65;81mOllama\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\u001B[38;2;55;65;81mstrict\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mFalse\u001B[0m\u001B[38;2;55;65;81m, \u001B[0m\n",
       "\u001B[38;2;55;65;81masync_mode\u001B[0m\u001B[38;2;55;65;81m=\u001B[0m\u001B[3;38;2;55;65;81mTrue\u001B[0m\u001B[1;38;2;55;65;81m)\u001B[0m\u001B[38;2;55;65;81m...\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because the answer correctly identifies that the Claude Models were built by Anthropic, a company known for its advanced AI technologies., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who built the Claude Models?\n",
      "  - actual output: OpenAI\n",
      "  - expected output: Claude Anthrophic\n",
      "  - context: None\n",
      "  - retrieval context: ['Claude Anthrophic built the GPT models.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 1.00 because the response directly addresses the question by identifying the current president, ensuring relevance and accuracy., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who is the current president of the United States of America?\n",
      "  - actual output: Joe Biden\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['Joe Biden serves as the current president of America.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel0d2yl093b2jlo3oumkzii/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel0d2yl093b2jlo3oumkzii/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel0d2yl093b2jlo3oumkzii/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[38;2;5;245;141m‚úì\u001B[0m Done üéâ! View results on \n",
       "\u001B]8;id=690274;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel0d2yl093b2jlo3oumkzii/test-cases\u001B\\\u001B[4;94mhttps://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel0d2yl093b2jlo3oumkzii/test-\u001B[0m\u001B]8;;\u001B\\\n",
       "\u001B]8;id=690274;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel0d2yl093b2jlo3oumkzii/test-cases\u001B\\\u001B[4;94mcases\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the answer correctly identifies that the Claude Models were built by Anthropic, a company known for its advanced AI technologies.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"OpenAI is an AI company.\",\\n    \"It specializes in developing advanced AI technologies.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Who built the Claude Models?', actual_output='OpenAI', expected_output='Claude Anthrophic', context=None, retrieval_context=['Claude Anthrophic built the GPT models.'], additional_metadata=None), TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response directly addresses the question by identifying the current president, ensuring relevance and accuracy.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"Joe Biden\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], conversational=False, multimodal=False, input='Who is the current president of the United States of America?', actual_output='Joe Biden', expected_output=None, context=None, retrieval_context=['Joe Biden serves as the current president of America.'], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmel0d2yl093b2jlo3oumkzii/test-cases')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.evaluate import evaluate\n",
    "\n",
    "answer_relevancy_metric = AnswerRelevancyMetric()\n",
    "\n",
    "test_case1 = LLMTestCase(\n",
    "  input=\"Who is the current president of the United States of America?\",\n",
    "  actual_output=\"Joe Biden\",\n",
    "  retrieval_context=[\"Joe Biden serves as the current president of America.\"]\n",
    ")\n",
    "\n",
    "test_case2 = LLMTestCase(\n",
    "  input=\"Who built the Claude Models?\",\n",
    "  actual_output=\"OpenAI\",\n",
    "  expected_output= \"Claude Anthrophic\",\n",
    "  retrieval_context=[\"Claude Anthrophic built the GPT models.\"]\n",
    ")\n",
    "\n",
    "evaluate(test_cases=[test_case1, test_case2], metrics=[answer_relevancy_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
