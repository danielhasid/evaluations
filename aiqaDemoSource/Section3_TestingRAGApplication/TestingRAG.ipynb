{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications üìë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Application\n",
    "This application reads data about Model Context Protocol (MCP) server from internet, stores in vector stores, chunks the data with embedding and useful to answer the question about MCP while inferenced.\n",
    "\n",
    "<img src=\"./img/RAG.png\" width=\"500\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add text to vector db\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    Give a summary not the full detail\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def retrieve_and_format(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_88397/3189513114.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP, or Model Context Protocol, is a protocol designed to facilitate interactions between large language models (LLMs) and various applications. It supports two main transport methods: STDIO for local integrations and HTTP+SSE for remote connections. Communication uses JSON-RPC 2.0 as the underlying message standard. The core components of MCP include the host application, which interacts with users; the MCP client, integrated within the host to handle connections; the MCP server, which adds context and exposes specific functions; and the transport layer, handling communication between clients and servers.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is MCP\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Application with DeepEval\n",
    "<img src=\"./img/RAGTesting.png\" width=\"800\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/c7p4f2sd36xbqwkp2djn8flc0000gn/T/ipykernel_24709/3189513114.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    }
   ],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What is MCP?\",\n",
    "    actual_output=chain.invoke(\"What is MCP\"),\n",
    "    expected_output=\"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps\"\n",
    ")\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "dataset.add_test_case(test_case=test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[LLMTestCase(input='What is MCP?', actual_output=\"MCP, or Model Context Protocol, is a protocol designed to facilitate interactions between large language models (LLMs) and various applications. It uses JSON-RPC 2.0 for standardized communication and supports two primary transport methods: STDIO for local integrations and HTTP+SSE for remote connections. MCP's architecture includes host applications that interact with users, MCP clients that handle communications with servers, and MCP servers that provide specific functions and context to AI apps. Overall, MCP enables seamless integration of LLMs into diverse applications by standardizing communication and providing a structured way to exchange data and context.\", expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps', context=None, retrieval_context=None, additional_metadata=None, tools_called=None, comments=None, expected_tools=None, token_cost=None, completion_time=None, name=None, tags=None, mcp_servers=None, mcp_tools_called=None, mcp_resources_called=None, mcp_prompts_called=None)], goldens=[], _alias=None, _id=None, _multi_turn=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "concise_metrics = GEval(\n",
    "    name = \"Concise\",\n",
    "    criteria=\"Assess if the actual output remains concise while preserving all essential information.\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "\n",
    "completness_metrics = GEval(\n",
    "    name = \"Completeness\",\n",
    "    criteria=\"Assess whether the actual output retains all the key information from the input\",\n",
    "    \n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with GEval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Completeness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mCompleteness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Concise </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using deepseek-r</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1:8b</span><span style=\"color: #374151; text-decoration-color: #374151\"> </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">Ollama</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mConcise \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing deepseek-r\u001b[0m\u001b[1;38;2;55;65;81m1:8b\u001b[0m\u001b[38;2;55;65;81m \u001b[0m\u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81mOllama\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Completeness [GEval] (score: 0.9, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The response effectively addresses all key aspects of MCP as outlined in the input, providing detailed information on communication protocols, transport methods, architecture components, and integration capabilities. It accurately retains the original meaning and context from the input., error: None)\n",
      "  - ‚úÖ Answer Relevancy (score: 0.8571428571428571, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The score is 0.86 because the output did not directly address what MCP stands for, instead discussing a transport method which is unrelated to the query., error: None)\n",
      "  - ‚úÖ Concise [GEval] (score: 0.9, threshold: 0.5, strict: False, evaluation model: deepseek-r1:8b (Ollama), reason: The actual output is concise and includes all essential information. It effectively explains MCP's purpose, components, and integration methods without unnecessary words. The response is clear and easy to understand., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is MCP?\n",
      "  - actual output: MCP, or Model Context Protocol, is a protocol designed to facilitate interactions between large language models (LLMs) and various applications. It uses JSON-RPC 2.0 for standardized communication and supports two primary transport methods: STDIO for local integrations and HTTP+SSE for remote connections. MCP's architecture includes host applications that interact with users, MCP clients that handle communications with servers, and MCP servers that provide specific functions and context to AI apps. Overall, MCP enables seamless integration of LLMs into diverse applications by standardizing communication and providing a structured way to exchange data and context.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Completeness [GEval]: 100.00% pass rate\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Concise [GEval]: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Done üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmem99ngh03lv8s4kgctm2boo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmem99ngh03lv8s4kgctm2boo/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmem99ngh03lv8s4kgctm2boo/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Done üéâ! View results on \n",
       "\u001b]8;id=990894;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmem99ngh03lv8s4kgctm2boo/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmem99ngh03lv8s4kgctm2boo/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=990894;https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmem99ngh03lv8s4kgctm2boo/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755832385.609059 13942971 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=True, metrics_data=[MetricData(name='Completeness [GEval]', threshold=0.5, success=True, score=0.9, reason='The response effectively addresses all key aspects of MCP as outlined in the input, providing detailed information on communication protocols, transport methods, architecture components, and integration capabilities. It accurately retains the original meaning and context from the input.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Criteria:\\nAssess whether the actual output retains all the key information from the input \\n \\nEvaluation Steps:\\n[\\n    \"Compare the actual output with the input to identify any missing or altered information.\",\\n    \"Identify and list all key pieces of information from both the input and the actual output.\",\\n    \"Verify that each identified key piece of information is present, accurate, and retains its original meaning in the actual output.\",\\n    \"Use a checklist or criteria list to systematically evaluate the retention of key information.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.9'), MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=0.8571428571428571, reason='The score is 0.86 because the output did not directly address what MCP stands for, instead discussing a transport method which is unrelated to the query.', strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Statements:\\n[\\n    \"MCP stands for Model Context Protocol.\",\\n    \"It is designed to facilitate interactions between large language models and various applications.\",\\n    \"MCP uses JSON-RPC 2.0 for standardized communication.\",\\n    \"It supports two primary transport methods: STDIO and HTTP+SSE.\",\\n    \"MCP\\'s architecture includes host applications, MCP clients, and MCP servers.\",\\n    \"Host applications interact with users, MCP clients handle communications with servers, and MCP servers provide specific functions and context to AI apps.\",\\n    \"Overall, MCP enables seamless integration of LLMs into diverse applications by standardizing communication and providing a structured way to exchange data and context.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"This statement describes a transport method, not directly answering what MCP is.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Concise [GEval]', threshold=0.5, success=True, score=0.9, reason=\"The actual output is concise and includes all essential information. It effectively explains MCP's purpose, components, and integration methods without unnecessary words. The response is clear and easy to understand.\", strict_mode=False, evaluation_model='deepseek-r1:8b (Ollama)', error=None, evaluation_cost=0.0, verbose_logs='Criteria:\\nAssess if the actual output remains concise while preserving all essential information. \\n \\nEvaluation Steps:\\n[\\n    \"Check if the actual output is concise without unnecessary words.\",\\n    \"Ensure all essential information is present and not omitted.\",\\n    \"Compare the actual output with a more concise version to verify preservation of key details.\",\\n    \"Assess clarity and readability, ensuring the output is easily understandable.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.9')], conversational=False, multimodal=False, input='What is MCP?', actual_output=\"MCP, or Model Context Protocol, is a protocol designed to facilitate interactions between large language models (LLMs) and various applications. It uses JSON-RPC 2.0 for standardized communication and supports two primary transport methods: STDIO for local integrations and HTTP+SSE for remote connections. MCP's architecture includes host applications that interact with users, MCP clients that handle communications with servers, and MCP servers that provide specific functions and context to AI apps. Overall, MCP enables seamless integration of LLMs into diverse applications by standardizing communication and providing a structured way to exchange data and context.\", expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps', context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmekcjvxy00e7e82whpp81o98/evaluation/test-runs/cmem99ngh03lv8s4kgctm2boo/test-cases')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.evaluate import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "evaluate(dataset.test_cases, metrics=[\n",
    "    completness_metrics, \n",
    "    AnswerRelevancyMetric(),\n",
    "    concise_metrics\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
