import json
import os

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# --- OpenAI setup ---
api_key = os.getenv("OPENAI_API_KEY")

def generate_evaluation_summary(json_file_path, api_key):
    # 1. טעינת התוצאות
    with open(json_file_path, 'r', encoding='utf-8') as f:
        eval_data = json.load(f)

    # 2. סינון והכנת הנתונים
    summary_data = []
    for item in eval_data:
        summary_data.append({
            "question": item.get("question", ""),
            "expected_answer": item.get("expected_answer", ""),
            "generated_answer": item.get("generated_answer", ""),
            "status": item.get("status", ""),
            "metadata_category": item.get("metadata", ""),
            "evaluation_metrics": item.get("evaluation_metrics", {})
        })

    # 3. הפרומפט המעודכן - פלט באנגלית, פירוט רק לנכשלים ומסקנות על הכל
    prompt = f"""
    You are an expert QA Evaluation Engineer for LLMs. 
    Analyze the following evaluation results generated by DeepEval and write a comprehensive report in English.

    The report MUST include the following sections:

    1. Overall Score & Status - **Based on ALL tests**:
       - Calculate the overall pass rate (percentage of passed tests out of the total).
       - Calculate the average score for the metrics evaluated across all tests.
       - Provide a short executive summary of the system's current overall performance.

    2. Weaknesses Analysis - **Focus ONLY on FAILED tests**:
       - Analyze the metrics specifically for the tests that failed (status: "failed").
       - Identify the core issues: Which metric causes the failures? 
       - Explain the pattern of these weaknesses (e.g., "The model fails in Hallucination when the expected answer is too short, causing it to over-explain").

    3. Detailed Breakdown of FAILED Tests ONLY:
       - For EVERY test in the JSON where "status" is "failed", provide:
         - The Question.
         - The specific metric scores it received.
         - A brief explanation of the failure based on comparing the 'expected_answer' vs 'generated_answer' and the metric's 'reason' field.
       - IMPORTANT: Do NOT include detailed breakdowns for tests that passed.

    4. Actionable Recommendations - **Based on ALL tests**:
       - Looking at the complete picture (both the tests that passed and the tests that failed), provide 2-3 concrete, actionable steps to improve.
       - Address whether the issue lies in the Model's prompts, the Dataset's ground truth (are the expected answers realistic compared to the passed tests?), or the Evaluation thresholds.

    Here is the JSON data of the test run:
    {json.dumps(summary_data, indent=2, ensure_ascii=False)}
    """

    # 4. קריאה ל-API
    client = OpenAI(api_key=api_key)
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system",
             "content": "You are a highly analytical AI assistant specialized in LLM evaluation analysis."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )

    return response.choices[0].message.content


# הרצת הפונקציה
if __name__ == "__main__":
    report = generate_evaluation_summary("evaluation_results.json", api_key)
    print(report)