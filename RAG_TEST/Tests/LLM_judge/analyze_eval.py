import json
import os

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# --- OpenAI setup ---
api_key = os.getenv("OPENAI_API_KEY")

def generate_evaluation_summary(json_file_path, api_key):
    # 1. Load the evaluation results
    with open(json_file_path, 'r', encoding='utf-8') as f:
        eval_data = json.load(f)

    # 2. Filter and prepare the data
    summary_data = []
    for item in eval_data:
        summary_data.append({
            "question": item.get("question", ""),
            "expected_answer": item.get("expected_answer", ""),
            "generated_answer": item.get("generated_answer", ""),
            "status": item.get("status", ""),
            "metadata_category": item.get("metadata", ""),
            "evaluation_metrics": item.get("evaluation_metrics", {})
        })

    # 3. The updated prompt
    prompt = f"""
    You are an expert QA Evaluation Engineer for LLMs. 
    Analyze the following evaluation results generated by DeepEval and write a comprehensive report in English.

    The report MUST include the following sections:

    1. Overall Score & Status - **Based on ALL tests**:
       - State the total number of tests, how many passed, and how many failed.
       - Calculate the overall pass rate. Provide ONLY the final percentage (e.g., 66.67%), do NOT show the calculation steps or mathematical formulas.
       - Calculate the average score for each metric evaluated across all tests. Provide ONLY the final average score number for each metric, do NOT show the math, fractions, or how you arrived at the number.
       - Provide a short executive summary of the system's current overall performance.

    2. Weaknesses Analysis - **Focus ONLY on FAILED tests**:
       - Analyze the metrics specifically for the tests that failed (status: "failed").
       - Identify the core issues: Which metric causes the failures? 
       - Explain the pattern of these weaknesses (e.g., "The model fails in Hallucination when the expected answer is too short, causing it to over-explain").

    3. Detailed Breakdown of FAILED Tests ONLY:
       - For EVERY test in the JSON where "status" is "failed", provide:
         - The Question.
         - The specific metric scores it received.
         - A brief explanation of the failure based on comparing the 'expected_answer' vs 'generated_answer' and the metric's 'reason' field.
       - IMPORTANT: Do NOT include detailed breakdowns for tests that passed.

    4. Actionable Recommendations - **Based on ALL tests**:
       - Looking at the complete picture (both the tests that passed and the tests that failed), provide 2-3 concrete, actionable steps to improve.
       - Address whether the issue lies in the Model's prompts, the Dataset's ground truth (are the expected answers realistic compared to the passed tests?), or the Evaluation thresholds.

    Here is the JSON data of the test run:
    {json.dumps(summary_data, indent=2, ensure_ascii=False)}
    """

    # 4. Call the API
    client = OpenAI(api_key=api_key)
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system",
             "content": "You are a highly analytical AI assistant specialized in LLM evaluation analysis."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1  # הורדתי מעט את הטמפרטורה כדי להבטיח ציות מחמיר להוראות הפורמט
    )

    return response.choices[0].message.content


# Run the function
if __name__ == "__main__":
    report = generate_evaluation_summary("evaluation_results.json", api_key)
    print(report)